%===============================================================================
% Sample bibliopgraphy file for ifaconf.bst style to be used in
% IFAC meeting papers
% Copyright (c) 2007-2008 International Federation of Automatic Control
%===============================================================================

% Used in Introduction section, background on Transformer architecture
@misc{vaswani2017attentionneed,
	title={Attention Is All You Need}, 
	author={Ashish Vaswani and Noam Shazeer and others},
	year={2017},
	eprint={1706.03762},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	note={\url{https://arxiv.org/abs/1706.03762}}, 
}

% Used in Introduction section, parameter to qaulity of output
@misc{touvron2023llamaopenefficientfoundation,
	title={LLaMA: Open and Efficient Foundation Language Models}, 
	author={Hugo Touvron and Thibaut Lavril and others},
	year={2023},
	eprint={2302.13971},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	note={\url{https://arxiv.org/abs/2302.13971}}, 
}


% Used in Introduction section, the required GPUs for foundational models
@misc{bommasani2022opportunitiesrisksfoundationmodels,
	title={On the Opportunities and Risks of Foundation Models}, 
	author={Rishi Bommasani and Drew A. Hudson and others},
	year={2022},
	eprint={2108.07258},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	note={\url{https://arxiv.org/abs/2108.07258}}, 
}

% =========================== Benchmarks ==================================
% HellaSwag
@misc{zellers2019hellaswagmachinereallyfinish,
	title={HellaSwag: Can a Machine Really Finish Your Sentence?}, 
	author={Rowan Zellers and Ari Holtzman and others},
	year={2019},
	eprint={1905.07830},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	note={\url{https://arxiv.org/abs/1905.07830}}, 
}

% WinoGrande
@misc{sakaguchi2019winograndeadversarialwinogradschema,
	title={WinoGrande: An Adversarial Winograd Schema Challenge at Scale}, 
	author={Keisuke Sakaguchi and Ronan Le Bras and others},
	year={2019},
	eprint={1907.10641},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	note={\url{https://arxiv.org/abs/1907.10641}}, 
}

% Arc
@misc{clark2018thinksolvedquestionanswering,
	title={Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge}, 
	author={Peter Clark and Isaac Cowhey and others},
	year={2018},
	eprint={1803.05457},
	archivePrefix={arXiv},
	primaryClass={cs.AI},
	note={\url{https://arxiv.org/abs/1803.05457}}, 
}

% OpenBookQA
@misc{mihaylov2018suitarmorconductelectricity,
	title={Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering}, 
	author={Todor Mihaylov and Peter Clark and others},
	year={2018},
	eprint={1809.02789},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	note={\url{https://arxiv.org/abs/1809.02789}}, 
}
% ======================== Quantizations ==================================
@misc{llamacpp,
	author = {Georgi Gerganov},
	title = {llama.cpp},
	year = {2023},
	publisher = {GitHub},
	journal = {GitHub repository},
	url = {\url{https://github.com/ggerganov/llama.cpp}},
	note = {\url{https://github.com/ggerganov/llama.cpp}}
}

@misc{ggml,
	author = {Georgi Gerganov},
	title = {ggml},
	year = {2022},
	publisher = {GitHub},
	journal = {GitHub repository},
	url = {\url{https://github.com/ggerganov/ggml}},
	note = {\url{https://github.com/ggerganov/ggml}}
}

@misc{lin2024awqactivationawareweightquantization,
	title={AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration}, 
	author={Ji Lin and Jiaming Tang and others},
	year={2024},
	eprint={2306.00978},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	note={\url{https://arxiv.org/abs/2306.00978}}, 
}

@misc{liu2024vptqextremelowbitvector,
	title={VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models}, 
	author={Yifei Liu and Jicheng Wen and others},
	year={2024},
	eprint={2409.17066},
	archivePrefix={arXiv},
	primaryClass={cs.AI},
	note={\url{https://arxiv.org/abs/2409.17066}}, 
}
% =========================================================================

% ============================== Selected LLMs ===========================
@misc{gemmateam2024gemma2improvingopen,
	title={Gemma 2: Improving Open Language Models at a Practical Size}, 
	author={Gemma Team and Morgane Riviere and others},
	year={2024},
	eprint={2408.00118},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	note={\url{https://arxiv.org/abs/2408.00118}}, 
}

@misc{dubey2024llama3herdmodels,
	title={The Llama 3 Herd of Models}, 
	author={Abhimanyu Dubey and Abhinav Jauhri and others},
	year={2024},
	eprint={2407.21783},
	archivePrefix={arXiv},
	primaryClass={cs.AI},
	note={\url{https://arxiv.org/abs/2407.21783}}, 
}

@misc{qwen2.5,
	title = {Qwen2.5: A Party of Foundation Models},
	note = {\url{https://qwenlm.github.io/blog/qwen2.5/}},
	author = {Qwen Team},
	month = {September},
	year = {2024}
}

% =========================================================================

@manual{raspberrypi4,
	title = {Raspberry Pi 4 Model B},
	organization = {Raspberry Pi Ltd},
	year = {2024},
	month = {4},
	note  = {\url{https://datasheets.raspberrypi.com/rpi4/raspberry-pi-4-product-brief.pdf}}
}

@misc{ggmlhuggingface,
	author = {Georgi Gerganov and Xuan Son Nguyen},
	title = {Introduction to ggml},
	year = 2024,
	note = {\url{https://huggingface.co/blog/introduction-to-ggml}}
}


@misc{ggmlgithubdocs,
	author = {Georgi Gerganov},
	title = {GGML},
	year = 2024,
	note = {\url{https://github.com/ggerganov/ggml/blob/master/docs/gguf.md}}
}


@misc{ggufgithub,
	author = {Georgi Gerganov},
	title = {GGUF},
	year = 2024,
	note = {\url{https://github.com/ggerganov/ggml}}
}

@misc{ggufgithubquantdoc,
	author = {Georgi Gerganov},
	title = {GGUF Quantize},
	year = 2024,
	note = {\url{https://github.com/ggerganov/llama.cpp/tree/master/examples/quantize}}
}

@misc{ggufgithubkquantpr,
	author = {Georgi Gerganov and Kawrakow},
	title = {GGUF K Quants Pull Request},
	year = 2024,
	note = {\url{https://github.com/ggerganov/llama.cpp/pull/1684}}
}

@misc{ggufgithubiquantpr,
	author = {Georgi Gerganov and Kawrakow},
	title = {GGUF I Quants Pull Request},
	year = 2024,
	note = {\url{https://github.com/ggerganov/llama.cpp/pull/4773}}
}

@misc{tseng2024quipbetterllmquantization,
	title={QuIP\#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks}, 
	author={Albert Tseng and Jerry Chee and Qingyao Sun and Volodymyr Kuleshov and Christopher De Sa},
	year={2024},
	eprint={2402.04396},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	note={\url{https://arxiv.org/abs/2402.04396}}, 
}

@misc{awqgithub,
	author = {MIT HAN Lab },
	title = {AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration},
	year = 2024,
	note = {\url{https://github.com/mit-han-lab/llm-awq}}
}

@misc{vptqgithub,
	author = {Microsoft},
	title = {VPTQ: Extreme Low-bit Vector Post-Training Quantization for Large Language Models},
	year = 2024,
	note = {\url{https://github.com/microsoft/VPTQ/blob/main/README.md}}
}

@misc{ma2023llmprunerstructuralpruninglarge,
	title={LLM-Pruner: On the Structural Pruning of Large Language Models}, 
	author={Xinyin Ma and Gongfan Fang and Xinchao Wang},
	year={2023},
	eprint={2305.11627},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	note={\url{https://arxiv.org/abs/2305.11627}}, 
}

@misc{dery2024everybodyprunenowstructured,
	title={Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes}, 
	author={Lucio Dery and Steven Kolawole and Jean-Fran√ßois Kagy and Virginia Smith and Graham Neubig and Ameet Talwalkar},
	year={2024},
	eprint={2402.05406},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	note={\url{https://arxiv.org/abs/2402.05406}}, 
}

% ========================== Benchmarks ===================================
@misc{wang2024mmluprorobustchallengingmultitask,
	title={MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark (Published at NeurIPS 2024 Track Datasets and Benchmarks)}, 
	author={Yubo Wang and Xueguang Ma and Ge Zhang and Yuansheng Ni and others},
	year={2024},
	eprint={2406.01574},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	note={\url{https://arxiv.org/abs/2406.01574}}, 
}

@misc{mmluprohuggingface,
	author = {Yubo Wang and Xueguang Ma and Ge Zhang and Yuansheng Ni and others},
	title = {MMLU-Pro Huggingface},
	year = 2024,
	note = {\url{https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro}}
}

@misc{zhong2023agievalhumancentricbenchmarkevaluating,
	title={AGIEval: A Human-Centric Benchmark for Evaluating Foundation Models}, 
	author={Wanjun Zhong and Ruixiang Cui and Yiduo Guo and others},
	year={2023},
	eprint={2304.06364},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	note={\url{https://arxiv.org/abs/2304.06364}}, 
}

@misc{lin2022truthfulqameasuringmodelsmimic,
	title={TruthfulQA: Measuring How Models Mimic Human Falsehoods}, 
	author={Stephanie Lin and Jacob Hilton and Owain Evans},
	year={2022},
	eprint={2109.07958},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	note={\url{https://arxiv.org/abs/2109.07958}}, 
}

@misc{zhou2023instructionfollowingevaluationlargelanguage,
	title={Instruction-Following Evaluation for Large Language Models}, 
	author={Jeffrey Zhou and Tianjian Lu and Swaroop Mishra and others},
	year={2023},
	eprint={2311.07911},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	note={\url{https://arxiv.org/abs/2311.07911}}, 
}
% =========================================================================